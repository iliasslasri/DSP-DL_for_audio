{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This is a simple implementation of the TDNN without subsampling described in the paper  \"A time delay neural network architecture for efficient modeling of long temporal contexts\" by Peddinti et al. (2015), and used in \"Time delay deep neural network-based universal background models for speaker recognition\" by Snyder et al. (2015). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = Namespace()\n",
    "\n",
    "param.T = 23 # the number of time frames\n",
    "param.dim_mfcc = 40 # the number of mfcc features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2(x):\n",
    "    x = x**2\n",
    "    x = torch.sum(x, 1)\n",
    "    x = torch.sqrt(x)\n",
    "    x = x.unsqueeze(1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TDNN, self).__init__()\n",
    "        self.layer1 = nn.Conv1d(1, 1, stride=1, kernel_size=5)\n",
    "        self.layer2 = nn.Conv1d(1, 1, stride=1, kernel_size=4)\n",
    "        self.layer3 = nn.Conv1d(1, 1, stride=1, kernel_size=7)\n",
    "        self.layer4 = nn.Conv1d(1, 1, stride=1, kernel_size=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = norm_2(x)\n",
    "        x = self.layer2(x)\n",
    "        x = norm_2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = norm_2(x)\n",
    "        x = self.layer4(x)\n",
    "        x = norm_2(x)\n",
    "        x = x.view(-1, x.size(1) * x.size(2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "tdnn = TDNN(param.dim_mfcc, 10)\n",
    "\n",
    "# mfcc features of 23 time steps, (1, 40) for each time step ( image of dimension (1, 23, 13) )\n",
    "x = torch.randn(param.dim_mfcc, 1, param.T)\n",
    "\n",
    "output = tdnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([40, 1, 23])\n",
      "output shape: torch.Size([40, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"input shape:\", x.size())\n",
    "\n",
    "print(\"output shape:\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The ouput of this TDNN is a vector featuring all the infomation in a temporal context window of 23 time steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
